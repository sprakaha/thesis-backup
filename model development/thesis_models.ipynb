{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rM5R4whhbJ1B"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bT-xAfQlZ9pa",
        "outputId": "fa4e283b-5d13-4a40-a620-e8c8d1e6bac4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jdIyVTyUVHMi"
      },
      "outputs": [],
      "source": [
        "# Import libraries \n",
        "import math\n",
        "import scipy\n",
        "import json\n",
        "import copy as cp\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "\n",
        "# Import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import feature_selection as fs\n",
        "from sklearn import preprocessing\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import linear_model\n",
        "\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from imblearn.over_sampling import ADASYN \n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBN_S2VsbH3K"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-YiafsZDW8Oh"
      },
      "outputs": [],
      "source": [
        "## Feature reduction via filtering approaches\n",
        "def anova_reduction(X, y, n, p):\n",
        "  # Select n features with the largest F-scores for y\n",
        "  fs_fit_fscore = fs.SelectKBest(fs.f_classif, k=n)\n",
        "  fs_fit_fscore.fit_transform(X, y)\n",
        "  fs_indices_fscore = np.argsort(np.nan_to_num(fs_fit_fscore.scores_))[::-1][0:n]\n",
        "  best_features_fscore = X.columns[fs_indices_fscore].values\n",
        "  if p:\n",
        "    print(\"Selected: \", best_features_fscore)\n",
        "  return X.iloc[:, fs_indices_fscore]\n",
        "\n",
        "def variance_reduction(X, cond):\n",
        "  # Drop features with less than cond variance \n",
        "  vars = X.var(axis=0)\n",
        "  names = vars.keys()\n",
        "  remove = []\n",
        "  for i in range(len(vars)):\n",
        "    if vars[i] < cond:\n",
        "      remove.append(names[i])\n",
        "  print(\"Removed: \", len(remove))\n",
        "  return X.drop(remove, axis = 1)\n",
        "\n",
        "def mutual_reduction(X, y, n, p):\n",
        "  # Selects n features with the highest mutual information to y\n",
        "  fs_fit_mutual_info = fs.SelectKBest(fs.mutual_info_classif, k=n)\n",
        "  fs_fit_mutual_info.fit_transform(X, y)\n",
        "  fs_indices_mutual_info = np.argsort(fs_fit_mutual_info.scores_)[::-1][0:n]\n",
        "  best_features_mutual_info = X.columns[fs_indices_mutual_info].values\n",
        "  if p: \n",
        "    print(\"Selected: \", best_features_mutual_info)\n",
        "  return X.iloc[:, fs_indices_mutual_info]\n",
        "\n",
        "def kendall_reduction(X, y, n, p):\n",
        "  # Select n features with the highest correlation to y \n",
        "  scores = []\n",
        "  for feat in X.columns:\n",
        "    # absolute for positive values\n",
        "    s = abs(scipy.stats.kendalltau(X[feat], y)[0])\n",
        "    scores.append(s)\n",
        "  relevant_kfeatures_i = np.argsort(scores)[0::n]\n",
        "  relevant_kfeatures = X.columns[relevant_kfeatures_i].values\n",
        "  if p: \n",
        "    print(\"Selected: \", relevant_kfeatures)\n",
        "  return X.iloc[:, relevant_kfeatures_i]\n",
        "  \n",
        "## Additional Preprocessing\n",
        "# Merging Classes that are less than or equal to the inputted score\n",
        "def mergeLowerClasses(df, score):\n",
        "  for s in ['score_Gr', 'score_Al', 'score_Fl']:\n",
        "    for i in range(df.shape[0]):\n",
        "      if df.at[i, s] <= score:\n",
        "        df.at[i, s] = score\n",
        "  return df\n",
        "\n",
        "# Floor values\n",
        "def floorYs(y):\n",
        "  y = [math.floor(s) for s in y]\n",
        "  return y "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kRAFWW7InMlT"
      },
      "outputs": [],
      "source": [
        "## Scoring \n",
        "def get_scores(yactual, yhat, met):\n",
        "  acc = accuracy_score(yactual, yhat)\n",
        "  f1 = f1_score(yactual, yhat, average=met)\n",
        "  re = recall_score(yactual, yhat, average=met)\n",
        "  pre = precision_score(yactual, yhat, average=met)\n",
        "  print(\"Accuracy: \", acc * 100)\n",
        "  for i in range(3):\n",
        "    print(\"Class \" + str(i + 3))\n",
        "    print(\"F1: \" + str(f1[i] * 100) + \", Recall: \" + str(re[i] * 100) + \", Precision: \" + str(pre[i] * 100))\n",
        "  return acc, f1, re, pre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_XcOElZxclQB"
      },
      "outputs": [],
      "source": [
        "# LOSO Prediction kNN \n",
        "def loso_predict_knn(model, X, y, r, s, ada, n):\n",
        "    model_ = cp.deepcopy(model)\n",
        "    cv = LeaveOneOut()\n",
        "    # enumerate splits\n",
        "    y_true, y_pred = list(), list()\n",
        "    for train_ix, test_ix in cv.split(X):\n",
        "      # split data - X is in a numpy array \n",
        "      X_train, X_test = X.iloc[train_ix, :], X.iloc[test_ix, :]\n",
        "      y_train, y_test = np.array(y)[train_ix], np.array(y)[test_ix]\n",
        "\n",
        "      if r == 'anova':\n",
        "        X_train = anova_reduction(X_train, y_train, s, False)\n",
        "      elif r == 'mutual':\n",
        "        X_train = mutual_reduction(X_train, y_train, s, False)\n",
        "      elif r == 'kendall':\n",
        "        X_train = kendall_reduction(X_train, y_train, s, False)\n",
        "\n",
        "      if ada:\n",
        "        ada = ADASYN(random_state=42, n_neighbors=n)\n",
        "        X_train, y_train = ada.fit_resample(X_train, y_train)\n",
        "\n",
        "      scaler = preprocessing.MinMaxScaler()\n",
        "      X_trainS = scaler.fit_transform(X_train)\n",
        "\n",
        "      # fit model\n",
        "      model_.fit(X_trainS, y_train)\n",
        "\n",
        "      # evaluate model\n",
        "      X_testR = X_test[X_train.columns]\n",
        "      X_testS = scaler.transform(X_testR)\n",
        "      yhat = model_.predict(X_testS)\n",
        "\n",
        "      # store\n",
        "      y_true.append(y_test[0])\n",
        "      y_pred.append(yhat[0])\n",
        "\n",
        "    return y_true, y_pred\n",
        "\n",
        "# LOSO Prediction Random Forest \n",
        "def loso_predict_rf(model, X, y, ada, n):\n",
        "    model_ = cp.deepcopy(model)\n",
        "    cv = LeaveOneOut()\n",
        "\n",
        "    # enumerate splits\n",
        "    y_true, y_pred = list(), list()\n",
        "\n",
        "    for train_ix, test_ix in cv.split(X):\n",
        "      # split data - X is in a dataframe\n",
        "      X_train, X_test = X.iloc[train_ix, :], X.iloc[test_ix, :]\n",
        "      y_train, y_test = np.array(y)[train_ix], np.array(y)[test_ix]\n",
        "\n",
        "      if ada:\n",
        "        ada = ADASYN(random_state=42, n_neighbors=n)\n",
        "        X_train, y_train = ada.fit_resample(X_train, y_train)\n",
        "\n",
        "      # fit model\n",
        "      model_.fit(X_train, y_train)\n",
        "\n",
        "      # evaluate model\n",
        "      yhat = model_.predict(X_test)\n",
        "\n",
        "      # store\n",
        "      y_true.append(y_test[0])\n",
        "      y_pred.append(yhat[0])\n",
        "\n",
        "    return y_true, y_pred\n",
        "\n",
        "# generic loso\n",
        "def losoEval2(model, X, y, metric):\n",
        "  cv = LeaveOneOut()\n",
        "  scores = cross_val_score(model, X, yGr, scoring=metric, cv=cv, n_jobs=-1)\n",
        "  # print(scores)\n",
        "  return np.mean(scores), np.std(scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2yN4ziFbOSi"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjUQ3JkEXSKR"
      },
      "outputs": [],
      "source": [
        "# Load in the data\n",
        "drtp = pd.read_csv('/content/drive/MyDrive/Thesis/Data/RTP_FeaturesV3.csv', index_col=[0])\n",
        "\n",
        "# Drop rows that have NaN\n",
        "drtp = drtp.dropna()\n",
        "drtp = drtp.reset_index(drop=True)\n",
        "\n",
        "# Generate combined\n",
        "comb = []\n",
        "for i in range(len(drtp['score_Gr'])):\n",
        "  tot = drtp['score_Al'][i] + drtp['score_Gr'][i] + drtp['score_Fl'][i]\n",
        "  if tot > 14:\n",
        "    comb.append(5)\n",
        "  elif  tot > 10.5:\n",
        "    comb.append(4)\n",
        "  else:\n",
        "    comb.append(3)\n",
        "  \n",
        "# Seperate X, y \n",
        "X = mergeLowerClasses(drtp, 3.5)  \n",
        "yGr = floorYs(drtp['score_Gr'])\n",
        "yAl\t= floorYs(drtp['score_Al']) \n",
        "yFl = floorYs(drtp['score_Fl'])\n",
        "subjects = drtp['subject']\n",
        "X = drtp.drop(['subject', 'move', 'score_Gr', 'score_Al', 'score_Fl', 'Unnamed: 0.1'], axis = 1)\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfEDFqZwNfst"
      },
      "source": [
        "# Feature Summary Statistics "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2L69D90HqO_"
      },
      "outputs": [],
      "source": [
        "Xstats = X.describe().drop('count')\n",
        "Xstats.sort_values(by = 'std', axis = 1, ascending = False)\n",
        "Xstats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvCszk7HJRQl"
      },
      "outputs": [],
      "source": [
        "Xstats_stats = Xstats.transpose().describe()\n",
        "Xstats_stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccCfyUvuJK8T"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "27640gi__tdP"
      },
      "outputs": [],
      "source": [
        "metric = comb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjwucz6nRVwo",
        "outputId": "3fecd0d2-c770-4cf4-ff62-56636563f337"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 26 folds for each of 72 candidates, totalling 1872 fits\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'class_weight': 'balanced',\n",
              " 'criterion': 'gini',\n",
              " 'max_depth': 4,\n",
              " 'max_features': 'sqrt',\n",
              " 'n_estimators': 10}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Grid search over all the hyperparemeters of interest for kNN\n",
        "params = { \n",
        "    'n_estimators': [10, 25, 50, 100],\n",
        "    'max_features': ['sqrt'],\n",
        "    'max_depth': [4,5,6],\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'class_weight': [None, 'balanced', 'balanced_subsample']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=RandomForestClassifier(),\n",
        "                           param_grid=params,\n",
        "                           cv = LeaveOneOut(),\n",
        "                           n_jobs=-1, verbose=1, scoring=\"accuracy\") #f1_macro\n",
        "\n",
        "grid_search.fit(X, metric)\n",
        "\n",
        "rf_best = grid_search.best_params_\n",
        "rf_best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5ZRg8pi7tE9"
      },
      "outputs": [],
      "source": [
        "# Training Evaluation\n",
        "rf_model_ = RandomForestClassifier(random_state=42, n_jobs=-1, max_depth=4, n_estimators=10, max_features='sqrt', criterion='gini', class_weight='balanced')\n",
        "rf_model_.fit(X, metric) \n",
        "ypreds = rf_model_.predict(X)\n",
        "confusion_matrix = metrics.confusion_matrix(metric, ypreds)\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = list(set(metric)))\n",
        "cm_display.plot()\n",
        "plt.show()\n",
        "\n",
        "ac, f1, re, pre = get_scores(metric, ypreds, None)\n",
        "print(\"Average F1: \", np.mean(f1))\n",
        "print(\"Average Recall: \", np.mean(re))\n",
        "print(\"Average Precision: \", np.mean(pre))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdG-eojn9CHW"
      },
      "outputs": [],
      "source": [
        "# Testing Evaluation \n",
        "rf_model1 = RandomForestClassifier(random_state=42, n_jobs=-1, max_depth=5, n_estimators=25, max_features='sqrt', criterion='gini', class_weight='balanced_subsample')\n",
        "ytrues, ypreds = loso_predict_rf(rf_model1, X, metric, False, 0) \n",
        "confusion_matrix = metrics.confusion_matrix(ytrues, ypreds)\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = list(set(ytrues)))\n",
        "cm_display.plot()\n",
        "plt.show()\n",
        "\n",
        "ac, f1, re, pre = get_scores(ytrues, ypreds, None)\n",
        "print(\"Average F1: \", np.mean(f1))\n",
        "print(\"Average Recall: \", np.mean(re))\n",
        "print(\"Average Precision: \", np.mean(pre))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydCpPK2e8_80"
      },
      "outputs": [],
      "source": [
        "# Random Forest w/Adasyn\n",
        "rf_model1 = RandomForestClassifier(random_state=42, n_jobs=-1, max_depth=5, n_estimators=10, max_features='sqrt', criterion='entropy', class_weight=None)\n",
        "ytrues, ypreds = loso_predict_rf(rf_model1, X, metric, True, 2) \n",
        "confusion_matrix = metrics.confusion_matrix(ytrues, ypreds)\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = list(set(ytrues)))\n",
        "cm_display.plot()\n",
        "plt.show()\n",
        "\n",
        "ac, f1, re, pre = get_scores(ytrues, ypreds, None)\n",
        "print(\"Average F1: \", np.mean(f1))\n",
        "print(\"Average Recall: \", np.mean(re))\n",
        "print(\"Average Precision: \", np.mean(pre))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Hs9EfhGAtw7"
      },
      "source": [
        "# Random Forest Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "HPTflsswo9qT"
      },
      "outputs": [],
      "source": [
        "## Random Forest Model Eval Wrapper \n",
        "\n",
        "def rf_interp_wrapper(model, x, Xtrain, Ytrain, metric):\n",
        "  '''\n",
        "  Inputs:\n",
        "  - model, data point to be predicted, and training data\n",
        "  -\n",
        "  Output: \n",
        "  - dictionary with the neigbors used to classify this point, \n",
        "  the class, and the most relevant features on the basis of gini impurity\n",
        "  '''\n",
        "  # Deep copy \n",
        "  model_ = cp.deepcopy(model)\n",
        "\n",
        "  # Get parameters\n",
        "  params = model_.get_params()\n",
        "\n",
        "  # Fit\n",
        "  model_.fit(Xtrain, Ytrain)\n",
        "  \n",
        "  # Prediction \n",
        "  yhat = model_.predict(x)\n",
        "  \n",
        "  # The features ranked by their relative importance\n",
        "  # Select the top 20\n",
        "  features =  Xtrain.columns\n",
        "  importance = model_.feature_importances_\n",
        "  rank = [x for _, x in sorted(zip(importance, features), reverse=True)][:20]\n",
        "  importance.sort()\n",
        "  importance = importance[::-1][:20]\n",
        "\n",
        "  # Return as a json \n",
        "  results = dict()\n",
        "  results[\"model\"] = params\n",
        "  results[\"metric\"] = metric\n",
        "  results[\"score\"] = str(yhat[0])\n",
        "  for i in range(len(rank)):\n",
        "    feat = rank[i]\n",
        "    info = dict()\n",
        "    info[\"importance\"] =  importance[i]\n",
        "    info[\"5mean\"] = Xtrain[feat].iloc[[i for i, x in enumerate(Ytrain) if x == 5]].mean()\n",
        "    info[\"5std\"] = Xtrain[feat].iloc[[i for i, x in enumerate(Ytrain) if x == 5]].std()\n",
        "\n",
        "    info[\"4mean\"] = Xtrain[feat].iloc[[i for i, x in enumerate(Ytrain) if x == 4]].mean()\n",
        "    info[\"4std\"] = Xtrain[feat].iloc[[i for i, x in enumerate(Ytrain) if x == 4]].std()\n",
        "\n",
        "    info[\"3mean\"] = Xtrain[feat].iloc[[i for i, x in enumerate(Ytrain) if x == 3]].mean()\n",
        "    info[\"3std\"] = Xtrain[feat].iloc[[i for i, x in enumerate(Ytrain) if x == 3]].std()\n",
        "    \n",
        "    results[feat] = info\n",
        "    \n",
        "  return results "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rjq8fTFVJMy9"
      },
      "outputs": [],
      "source": [
        "# Running wrapper evalution on model for subject 1\n",
        "ans = rf_interp_wrapper(rf_model1, X.iloc[:1], X.iloc[1:], comb[1:], \"comb\")\n",
        "\n",
        "with open(\"rf_explanation.json\", \"w\") as fp:\n",
        "    json.dump(ans,fp) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msWgOSIUkwzp"
      },
      "source": [
        "## kNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "237DlW3hbCfv"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGAtSu51llCa"
      },
      "outputs": [],
      "source": [
        "# GridSearch over all the hyperparameters of interest for kNN\n",
        "def knn_gridsearch(X, y):\n",
        "  maxScore = [0, 0, 0, 0, 0, 0, 0, 0]\n",
        "  for s in range(5, 13):\n",
        "      for r in ['anova', 'mutual', 'kendall']:\n",
        "        for n in [2, 3, 4, 5]:\n",
        "          for d in ['euclidean', 'minkowski', 'chebyshev', 'manhattan']:\n",
        "            for w in ['uniform', 'distance']:\n",
        "              # Create classifier\n",
        "              knn_model = KNeighborsClassifier(n_neighbors = n, metric=d, weights=w)\n",
        "              # Get mean accuracy on the entered values\n",
        "              for val in [False]:\n",
        "                ytrues, ypreds = loso_predict_knn(knn_model, X, y, r, s, val, 3) \n",
        "                acc, f1  =  accuracy_score(ytrues, ypreds), f1_score(ytrues, ypreds, average=None)\n",
        "                if acc > maxScore[0]:\n",
        "                  maxScore[0] = acc\n",
        "                  maxScore[1] = f1\n",
        "                  maxScore[2] = n\n",
        "                  maxScore[3] = d\n",
        "                  maxScore[4] = r\n",
        "                  maxScore[5] = s\n",
        "                  maxScore[6] = w\n",
        "                  maxScore[7] = val\n",
        "                \n",
        "\n",
        "  print(maxScore)\n",
        "\n",
        "metric2 = yFl\n",
        "knn_gridsearch(X, metric2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJ9Kukhz-tY5"
      },
      "outputs": [],
      "source": [
        "# Training kNN Evalution\n",
        "knn_model_ = KNeighborsClassifier(n_neighbors = 2, metric='chebyshev', weights='uniform')\n",
        "knn_model_.fit(X, metric2)\n",
        "ypreds = knn_model_.predict(X)\n",
        "confusion_matrix = metrics.confusion_matrix(metric2, ypreds)\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = list(set(metric2)))\n",
        "cm_display.plot()\n",
        "plt.show()\n",
        "\n",
        "ac, f1, re, pre = get_scores(metric2, ypreds, None)\n",
        "print(\"Average F1: \", np.mean(f1))\n",
        "print(\"Average Recall: \", np.mean(re))\n",
        "print(\"Average Precision: \", np.mean(pre))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2zVYQf1475u"
      },
      "outputs": [],
      "source": [
        "# Testing kNN \n",
        "metric2 = comb\n",
        "knn_model = KNeighborsClassifier(n_neighbors = 3, metric='chebyshev', weights='uniform')\n",
        "ytrues, ypreds = loso_predict_knn(knn_model, X, metric2, 'kendall', 8, False, 3) \n",
        "confusion_matrix = metrics.confusion_matrix(ytrues, ypreds)\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = list(set(ytrues)))\n",
        "cm_display.plot()\n",
        "plt.show()\n",
        "\n",
        "ac, f1, re, pre = get_scores(ytrues, ypreds, None)\n",
        "print(\"Average F1: \", np.mean(f1))\n",
        "print(\"Average Recall: \", np.mean(re))\n",
        "print(\"Average Precision: \", np.mean(pre))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ha0tpArRwKxM"
      },
      "outputs": [],
      "source": [
        "# Testing kNN w/Adasyn\n",
        "knn_model2 = KNeighborsClassifier(n_neighbors = 5, metric='chebyshev', weights='uniform')\n",
        "ytrues, ypreds = loso_predict_knn(knn_model2, X, metric,'kendall', 6, True, 2) \n",
        "confusion_matrix = metrics.confusion_matrix(ytrues, ypreds)\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = list(set(ytrues)))\n",
        "cm_display.plot()\n",
        "plt.show()\n",
        "\n",
        "ac, f1, re, pre = get_scores(ytrues, ypreds, None)\n",
        "print(\"Average F1: \", np.mean(f1))\n",
        "print(\"Average Recall: \", np.mean(re))\n",
        "print(\"Average Precision: \", np.mean(pre))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHGJD3mIAwR5"
      },
      "source": [
        "# kNN Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ysSCmHdHwd_S"
      },
      "outputs": [],
      "source": [
        "## kNN Model Eval Wrapper \n",
        "def knn_interp_wrapper(model, x, Xtrain, Ytrain, metric, r, s):\n",
        "  '''\n",
        "  Inputs:\n",
        "  - model, data point to be predicted, and training data\n",
        "  -\n",
        "  Output: \n",
        "  - dictionary with the neighbors used to classify this point, \n",
        "  the class, and the most relevant features \n",
        "  '''\n",
        "  # Deep copy \n",
        "  model_ = cp.deepcopy(model)\n",
        "\n",
        "  # Get parameters\n",
        "  params = model_.get_params()\n",
        "\n",
        "  # Fit\n",
        "  if r == 'anova':\n",
        "    X_train = anova_reduction(Xtrain, Ytrain, s, False)\n",
        "  elif r == 'mutual':\n",
        "    X_train = mutual_reduction(Xtrain, Ytrain, s, False)\n",
        "  elif r == 'kendall':\n",
        "    X_train = kendall_reduction(Xtrain, Ytrain, s, False)\n",
        "\n",
        "  scaler = preprocessing.MinMaxScaler()\n",
        "  XX = scaler.fit_transform(X_train)\n",
        "  model_.fit(XX, Ytrain)\n",
        "  \n",
        "  # Prediction \n",
        "  xx = scaler.transform(x[X_train.columns])\n",
        "  yhat = model_.predict(xx)\n",
        "  \n",
        "  # Relative falling of the k closest neighbors \n",
        "  dist, neighs = model_.kneighbors(xx)\n",
        "\n",
        "  # Return as a json \n",
        "  results = dict()\n",
        "  results[\"model\"] = params\n",
        "  results[\"metric\"] = metric\n",
        "  results[\"score\"] = str(yhat[0])\n",
        "  results[\"results\"] = dict()\n",
        "  for i in range(neighs.size):\n",
        "    neigh = neighs[:, i][0]\n",
        "    # loop over all the features to find the closest on that plane \n",
        "    distances = []\n",
        "    features = X_train.columns\n",
        "    for col in range(len(features)):\n",
        "      distances.append(abs((xx[:, col] - XX[neigh, col])[0]))\n",
        "    rank = [x for _, x in sorted(zip(distances, features))]\n",
        "    distances.sort()\n",
        "    feats = {\n",
        "        \"dist\": dist[:, i][0],\n",
        "        \"feat_rank\": rank, \n",
        "        \"feat_list\": distances, \n",
        "        \"label\": Ytrain[neigh]\n",
        "    }\n",
        "    results[\"results\"][str(neigh)] = feats\n",
        "\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "yOGs-DBhwo38"
      },
      "outputs": [],
      "source": [
        "# Running wrapper evalution on model for subject 1\n",
        "ans = knn_interp_wrapper(knn_model, X.iloc[:1], X.iloc[1:], comb[1:], \"comb\", \"kendall\", 8)\n",
        "\n",
        "with open(\"knn_explanation.json\", \"w\") as fp:\n",
        "    json.dump(ans,fp) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owcMiN5rdGSA",
        "outputId": "1548a684-dfbe-4141-e7a7-bae4dbea5e78"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "comb[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkmU3Rn6d3Zj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0339e5925d00d506ad7d4fad2d0e073fef669dd1d2028ccbe27ea59fadca371"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
